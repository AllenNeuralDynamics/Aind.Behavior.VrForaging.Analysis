{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython magic tools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# Plotting and data managing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sns.set_context('talk')\n",
    "\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None  # Ignore SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "pdf_path = r'Z:\\scratch\\vr-foraging\\sessions'\n",
    "base_path = r'Z:\\scratch\\vr-foraging\\data'\n",
    "data_path = r'../../../data/'\n",
    "results_path = r'C:\\Users\\tiffany.ona\\OneDrive - Allen Institute\\Documents\\VR foraging\\experiments\\batch 4 - manipulating cost of travelling and global statistics\\results'\n",
    "\n",
    "# Modelling libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "# Statistical tools\n",
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_mouse_scaled, y_mouse):\n",
    "                    # Define the parameter grid\n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "    # Initialize the logistic regression model\n",
    "    log_reg = LogisticRegression()\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_mouse_scaled, y_mouse)\n",
    "\n",
    "    # Get the best parameter\n",
    "    best_C = grid_search.best_params_['C']\n",
    "    print(f\"The best value for C is: {best_C}\")\n",
    "\n",
    "    # Get the best score\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"The best cross-validation score is: {best_score:.2f}\")\n",
    "\n",
    "    # Perform 5-fold cross-validation\n",
    "    log_reg = LogisticRegression(C=best_C)\n",
    "    \n",
    "    return best_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(metrics_list, y_mouse, y_pred):    # Calculate confusion matrix (TP, TN, FP, FN)\n",
    "    \"\"\"\n",
    "    Calculate various classification metrics and append them to the provided metrics list.\n",
    "    Parameters:\n",
    "    metrics_list (list): A list to which the calculated metrics dictionary will be appended.\n",
    "    y_mouse (array-like): True labels.\n",
    "    y_pred (array-like): Predicted labels.\n",
    "    Returns:\n",
    "    list: The updated metrics list with the metrics dictionary for the current fold.\n",
    "    \n",
    "    The metrics dictionary contains the following keys:\n",
    "    - \"Accuracy 0\": Accuracy for class 0 (negative class).\n",
    "    - \"Precision 0\": Precision for class 0 (negative class).\n",
    "    - \"Recall 0\": Recall for class 0 (negative class).\n",
    "    - \"F1 Score 0\": F1 score for class 0 (negative class).\n",
    "    - \"Accuracy 1\": Accuracy for class 1 (positive class).\n",
    "    - \"Precision 1\": Precision for class 1 (positive class).\n",
    "    - \"Recall 1\": Recall for class 1 (positive class).\n",
    "    - \"F1 Score 1\": F1 score for class 1 (positive class).\n",
    "    - \"TN\": True negatives.\n",
    "    - \"FP\": False positives.\n",
    "    - \"FN\": False negatives.\n",
    "    - \"TP\": True positives.\n",
    "    \"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_mouse, y_pred)\n",
    "    TP = cm[1, 1]\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    \n",
    "    # Calculate metrics for class 0 (negative class)\n",
    "    precision_0 = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    recall_0 = TN / (TN + FN) if (TN + FN) > 0 else 0\n",
    "    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0\n",
    "    accuracy_0 = (TN) / (TN + FP)  # Proportion of predictions that were `0`\n",
    "    \n",
    "    # Calculate metrics for class 1 (positive class)\n",
    "    precision_1 = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    recall_1 = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n",
    "    accuracy_1 = (TP) / (TP + FN)  # Proportion of predictions that were `1`\n",
    "    \n",
    "    # Collect the metrics for this fold as a dictionary\n",
    "    fold_metrics = {\n",
    "        \"Accuracy 0\": accuracy_0,\n",
    "        \"Precision 0\": precision_0,\n",
    "        \"Recall 0\": recall_0,\n",
    "        \"F1 Score 0\": f1_0,\n",
    "        \"Accuracy 1\": accuracy_1,\n",
    "        \"Precision 1\": precision_1,\n",
    "        \"Recall 1\": recall_1,\n",
    "        \"F1 Score 1\": f1_1,\n",
    "        \"TN\": TN,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"TP\": TP\n",
    "    }\n",
    "    \n",
    "    # Append the metrics dictionary to the list\n",
    "    metrics_list.append(fold_metrics)\n",
    "    return metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_roc_curve(y_probs, y_mouse, plot=False):\n",
    "    # Assuming log_reg is your trained logistic regression model\n",
    "    # and X_mouse_scaled is your test data (or any data to predict on)\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_mouse, y_probs)\n",
    "\n",
    "    # Calculate AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Find the best threshold (maximizing Youden's J statistic)\n",
    "    # J = TPR - FPR\n",
    "    j_scores = tpr - fpr\n",
    "    best_threshold_index = np.argmax(j_scores)\n",
    "    best_threshold = thresholds[best_threshold_index]\n",
    "\n",
    "    if plot:\n",
    "        # Plot ROC curve\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Random classifier (diagonal line)\n",
    "        plt.scatter(fpr[best_threshold_index], tpr[best_threshold_index], color='red', label=f'Best threshold = {best_threshold:.2f}')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        sns.despine()\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"Best threshold: {best_threshold}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_session(summary_df, \n",
    "                              use_polynomial_features=True, \n",
    "                              orig_features = ['reward_probability', 'consecutive_failures', 'visit_number', 'cumulative_rewards', 'active_patch']):\n",
    "\n",
    "    # Initialize dataframes to store weights and cross-validation results\n",
    "    weights_df = pd.DataFrame(columns=['regressors', 'weights', 'mouse', 'session'])\n",
    "    cv_results_df = pd.DataFrame()\n",
    "    metrics_list = []\n",
    "    new_mouse_df = pd.DataFrame()\n",
    "    \n",
    "    for (mouse, session), mouse_df in summary_df.groupby(['mouse', 'session']):\n",
    "        print(f\"Mouse: {mouse}, Session: {session}\")\n",
    "        \n",
    "        # Select features and target variable\n",
    "        X_mouse = mouse_df[orig_features]\n",
    "        y_mouse = mouse_df['has_choice'].astype(int)\n",
    "        \n",
    "        if 'odor_label' in orig_features:\n",
    "            X_mouse = pd.get_dummies(X_mouse, columns=['odor_label'])\n",
    "            \n",
    "        # Define the pipeline\n",
    "        if use_polynomial_features:\n",
    "            poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "            X_mouse = poly.fit_transform(X_mouse)\n",
    "            features = poly.get_feature_names_out()\n",
    "        else:\n",
    "            features = X_mouse.columns\n",
    "        \n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_mouse_scaled = scaler.fit_transform(X_mouse)\n",
    "        \n",
    "        # Perform 5-fold cross-validation\n",
    "        if len(X_mouse_scaled) < 20:\n",
    "            continue\n",
    "        \n",
    "        if y_mouse.nunique() == 1:\n",
    "            continue   \n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True)  # random_state ensures reproducibility\n",
    "        log_reg = LogisticRegression(C=1, class_weight='balanced')\n",
    "        cv_scores = cross_val_score(log_reg, X_mouse_scaled, y_mouse, cv=cv, scoring='roc_auc')\n",
    "\n",
    "        # Fit the logistic regression model using formula\n",
    "        log_reg.fit(X_mouse_scaled, y_mouse)\n",
    "\n",
    "        # Predict class labels (0 or 1)\n",
    "        y_pred = log_reg.predict(X_mouse_scaled)\n",
    "        mouse_df['y_pred'] = y_pred\n",
    "        \n",
    "        y_probs = log_reg.predict_proba(X_mouse_scaled)[:, 1]\n",
    "        mouse_df['y_pred_prob'] = y_probs\n",
    "        \n",
    "        best_threshold = plotting_roc_curve(y_probs, y_mouse)\n",
    "        \n",
    "        y_pred_adjusted = (y_probs >= best_threshold).astype(int)\n",
    "        mouse_df['y_pred_adjusted'] = y_pred_adjusted\n",
    "        \n",
    "        mouse_df['norm_active_patch'] = mouse_df['active_patch'] / mouse_df['active_patch'].max()\n",
    "        metrics_list = calculate_metrics(metrics_list, y_mouse, y_pred_adjusted)\n",
    "        \n",
    "        feature_weights = pd.Series(log_reg.coef_[0], index=features)\n",
    "        feature_weights = feature_weights.reset_index()\n",
    "        feature_weights.rename(columns={'index': 'regressors', 0: 'weights'}, inplace=True)\n",
    "        feature_weights['mouse'] = mouse\n",
    "        feature_weights['session'] = session\n",
    "\n",
    "        # Append the weights and cv scores to the respective dataframes\n",
    "        weights_df = pd.concat([weights_df, feature_weights], ignore_index=True)\n",
    "        cv_results_df = pd.concat([cv_results_df, pd.DataFrame({'session': [session], 'mouse': [mouse], 'cv_std': [cv_scores.std()],\n",
    "                                                                'cv_score': [cv_scores.mean()]})], ignore_index=True)\n",
    "        \n",
    "        new_mouse_df = pd.concat([new_mouse_df, mouse_df], ignore_index=True)\n",
    "\n",
    "    weights_df['mouse'] = weights_df['mouse'].round(0).astype(str)\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return weights_df, cv_results_df, metrics_df, new_mouse_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename= 'batch_4.csv', interpatch_name = 'PostPatch'):\n",
    "    if filename == 'batch_4.csv':\n",
    "        experiment_list = ['data_collection', 'friction', 'control', 'distance_long', 'distance_short', 'friction_low','friction_med', 'friction_high', 'distance_extra_long', 'distance_extra_short']\n",
    "    else:\n",
    "        experiment_list = ['base', 'experiment1', 'experiment2']\n",
    "        \n",
    "    print('Loading')\n",
    "    summary_df = pd.read_csv(os.path.join(data_path, filename), index_col=0)\n",
    "\n",
    "    summary_df = summary_df[(summary_df['mouse'] != 754573)&(summary_df['mouse'] != 754572)]\n",
    "\n",
    "    summary_df = summary_df.loc[summary_df.experiment.isin(experiment_list)]\n",
    "    \n",
    "    summary_df['END'] = summary_df.index.to_series().shift(-1)\n",
    "    summary_df['START'] =  summary_df.index\n",
    "    summary_df['duration_epoch'] = summary_df['END'] - summary_df['START']\n",
    "\n",
    "    # Fill in missing values in active_patch\n",
    "    summary_df['active_real'] = summary_df['active_patch'].shift(-1)\n",
    "    summary_df['active_patch'] = np.where(summary_df['label'] == 'PostPatch', summary_df['active_real'], summary_df['active_patch'])\n",
    "    \n",
    "    ## Add interpatch time and distance as new columns\n",
    "    df = summary_df.loc[summary_df.label == interpatch_name].groupby(['mouse','session', 'active_patch'], as_index=False).agg({'length': 'mean', 'duration_epoch': 'first'})\n",
    "    df.rename(columns={'length':'interpatch_length', 'duration_epoch': 'interpatch_time'}, inplace=True)\n",
    "    summary_df = summary_df.merge(df, on=['mouse','session', 'active_patch'], how='left')\n",
    "\n",
    "    summary_df = summary_df.loc[(summary_df.label == 'RewardSite')]\n",
    "    # summary_df = summary_df.loc[(summary_df['odor_label'] != 'Amyl Acetate')]\n",
    "    summary_df = summary_df.loc[(summary_df['active_patch'] <= 20)|(summary_df['engaged'] ==True)]\n",
    "\n",
    "    return  summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage distribution of stops/leaves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "sns.barplot(data=summary_df, x='mouse', y='has_choice')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fit each session and mouse independently**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run model without any interaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = load()\n",
    "epoch = 'control'\n",
    "summary_df = summary_df.loc[(summary_df.experiment == epoch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'roc_auc'\n",
    "features = ['reward_probability','consecutive_failures', 'visit_number', 'cumulative_rewards', 'active_patch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df, cv_results_df, metrics_df, new_mouse_df = logistic_session(summary_df, \n",
    "                              use_polynomial_features=False, \n",
    "                              orig_features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "new_mouse_df['score'] = np.where(new_mouse_df['y_pred'] == new_mouse_df['has_choice'], 1, 0)\n",
    "results = new_mouse_df.groupby(['mouse', 'norm_active_patch', 'has_choice']).agg({'score': 'mean'}).reset_index()\n",
    "results['norm_active_patch'] = results['norm_active_patch'].round(2)\n",
    "sns.lineplot(data=results, x='norm_active_patch', y='score', hue='has_choice', palette={True: sns.color_palette()[0], False: sns.color_palette()[1]}, legend=False)\n",
    "# sns.lineplot(data=results, x='norm_active_patch', y='has_choice')\n",
    "sns.despine()\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the metrics of the fit\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "for column, ax in zip(['Accuracy 1', 'Precision 1', 'Recall 1', 'F1 Score 1'], axes.flatten()):\n",
    "    sns.histplot(data=metrics_df,  x =column, ax=ax, bins=np.arange(0, 1.1, 0.02), label='stop')\n",
    "for column, ax in zip(['Accuracy 0', 'Precision 0', 'Recall 0', 'F1 Score 0'], axes.flatten()):\n",
    "    sns.histplot(data=metrics_df,  x =column, ax=ax, bins=np.arange(0, 1.1, 0.02), label='leave')\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check distributions of scores and std\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.suptitle(scoring)\n",
    "sns.histplot(data=cv_results_df, x='cv_score',  bins=30,  ax=ax[0], legend=False)\n",
    "sns.histplot(data=cv_results_df, x='cv_std',  bins=30,  ax=ax[1])\n",
    "ax[0].set_xlabel('Cross-validation scores')\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare the predictions with the actual values\n",
    "plot_df = new_mouse_df.groupby(['mouse', 'session', 'has_choice']).score.mean().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for mouse, ax in zip(plot_df.mouse.unique(), axes.flatten()):\n",
    "    sns.barplot(data=plot_df.loc[plot_df.mouse == mouse], y='score', hue='has_choice', ax=ax, legend=False)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f'Mouse {mouse}')\n",
    "# Manually create the legend\n",
    "handles = [mpatches.Patch(color=sns.color_palette()[i], label=label) for i, label in enumerate([0,1])]\n",
    "fig.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare the predictions with the actual values averaged all mice\n",
    "plot_df = new_mouse_df.groupby(['mouse',  'has_choice']).score.mean().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "sns.barplot(data=plot_df, y='score', hue='has_choice', ax=ax, palette={True: sns.color_palette()[0], False: sns.color_palette()[1]}, dodge=True, legend=False)\n",
    "ax.set_ylim(0, 1)\n",
    "# Manually create the legend\n",
    "handles = [mpatches.Patch(color=sns.color_palette()[i], label=label) for i, label in enumerate([0,1])]\n",
    "fig.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot animal per animal weights of the coeficients\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 14), sharex=True)\n",
    "palette = {regressor: color for regressor, color in zip(weights_df['regressors'].unique(), sns.color_palette('tab10', len(weights_df['regressors'].unique())))}\n",
    "\n",
    "# Perform t-tests and plot significance\n",
    "for (mouse, group), ax in zip(weights_df.groupby('mouse'), axes.flatten()):\n",
    "    # Perform t-test for each regressor in the group\n",
    "    significant_regressors = []\n",
    "    for regressor in group['regressors'].unique():\n",
    "        regressor_data = group[group['regressors'] == regressor]['weights']\n",
    "        t_stat, p_value = ttest_1samp(regressor_data, 0)\n",
    "        \n",
    "        # Determine the significance level\n",
    "        if p_value < 0.001:\n",
    "            significance = '***'\n",
    "        elif p_value < 0.01:\n",
    "            significance = '**'\n",
    "        elif p_value < 0.05:\n",
    "            significance = '*'\n",
    "        else:\n",
    "            significance = None\n",
    "\n",
    "        if significance:\n",
    "            significant_regressors.append((regressor, regressor_data.max(), significance))\n",
    "\n",
    "    # Plot the swarmplot\n",
    "    sns.swarmplot(\n",
    "        data=group, \n",
    "        x='regressors', \n",
    "        y='weights', \n",
    "        palette=palette, \n",
    "        ax=ax, \n",
    "        hue='regressors', \n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(f'Mouse {mouse}')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.hlines(0, -0.5, len(group['regressors'].unique()) - 0.5, color='black', linestyle='--')\n",
    "\n",
    "    # Annotate significant results\n",
    "    for regressor, max_value, significance in significant_regressors:\n",
    "        x = list(group['regressors'].unique()).index(regressor)\n",
    "        y = max_value + 0.05  # Position above max value\n",
    "        ax.text(x, y, significance, ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "# Manually create the legend\n",
    "handles = []\n",
    "for regressor, color in palette.items():\n",
    "    handles.append(mpatches.Patch(color=color, label=regressor))\n",
    "\n",
    "# Add legend at the bottom with 3 columns\n",
    "fig.legend(\n",
    "    handles=handles,\n",
    "    bbox_to_anchor=(0.6, 0.05),  # Centered below the figure\n",
    "    loc='upper center',\n",
    "    ncol=3,  # Number of columns\n",
    "    title='Features',\n",
    "    prop={'size': 12}\n",
    ")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust()  # Add space at the bottom for the legend\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(results_path, f'weights_per_mouse_small_model_{epoch}.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the weights by mouse and regressor\n",
    "aggregated_df = weights_df.groupby(['mouse', 'regressors'], as_index=False).weights.mean()\n",
    "\n",
    "# Perform t-tests on the aggregated data\n",
    "t_test_results = []\n",
    "for regressor in aggregated_df['regressors'].unique():\n",
    "    regressor_data = aggregated_df[aggregated_df['regressors'] == regressor]['weights']\n",
    "    t_stat, p_value = ttest_1samp(regressor_data, 0)\n",
    "    \n",
    "    # Determine the significance level\n",
    "    if p_value < 0.001:\n",
    "        significance = '***'\n",
    "    elif p_value < 0.01:\n",
    "        significance = '**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = '*'\n",
    "    else:\n",
    "        significance = None\n",
    "\n",
    "    t_test_results.append({'regressor': regressor, 'p_value': p_value, 'significance': significance})\n",
    "\n",
    "t_test_results_df = pd.DataFrame(t_test_results)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# One point per mouse\n",
    "sns.swarmplot(\n",
    "    data=aggregated_df, \n",
    "    x='regressors', \n",
    "    y='weights', \n",
    "    hue='regressors', \n",
    "    palette=palette, \n",
    "    dodge=True\n",
    ")\n",
    "\n",
    "# Annotate significance levels\n",
    "for i, row in t_test_results_df.iterrows():\n",
    "    regressor = row['regressor']\n",
    "    significance = row['significance']\n",
    "    if significance:\n",
    "        x = list(aggregated_df['regressors'].unique()).index(regressor)\n",
    "        y = aggregated_df[aggregated_df['regressors'] == regressor]['weights'].max() + 0.1\n",
    "        plt.text(x, y, significance, ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "# Add horizontal line at 0\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "\n",
    "# Customize labels and legend\n",
    "plt.xlabel('')\n",
    "plt.xlim(-1, len(aggregated_df['regressors'].unique()))\n",
    "plt.ylabel('Weight')\n",
    "plt.xticks([])\n",
    "plt.title('Weights Per Regressor \\n (Aggregated by Mouse)')\n",
    "\n",
    "# Manually create legend\n",
    "plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(os.path.join(results_path, f'weights_all_small_model_{epoch}.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run model with interactions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df, cv_results_df, metrics_df, new_mouse_df = logistic_session(summary_df, \n",
    "                              use_polynomial_features=True, \n",
    "                              orig_features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "for column, ax in zip(['Accuracy 1', 'Precision 1', 'Recall 1', 'F1 Score 1'], axes.flatten()):\n",
    "    sns.histplot(data=metrics_df,  x =column, ax=ax, bins=np.arange(0, 1.1, 0.02), label='stop')\n",
    "for column, ax in zip(['Accuracy 0', 'Precision 0', 'Recall 0', 'F1 Score 0'], axes.flatten()):\n",
    "    sns.histplot(data=metrics_df,  x =column, ax=ax, bins=np.arange(0, 1.1, 0.02), label='leave')\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.suptitle(scoring)\n",
    "sns.histplot(data=cv_results_df, x='cv_score', multiple='stack', bins=30, color='black', ax=ax[0])\n",
    "sns.histplot(data=cv_results_df, x='cv_std', multiple='stack', bins=30, color='black', ax=ax[1])\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting animals separately\n",
    "fig, axes = plt.subplots(2, 5, figsize=(26, 8), sharex=True)\n",
    "\n",
    "# Perform t-tests and plot significance\n",
    "for (mouse, group), ax in zip(weights_df.groupby('mouse'), axes.flatten()):\n",
    "    # Perform t-test for each regressor in the group\n",
    "    significant_regressors = []\n",
    "    for regressor in group['regressors'].unique():\n",
    "        regressor_data = group[group['regressors'] == regressor]['weights']\n",
    "        t_stat, p_value = ttest_1samp(regressor_data, 0)\n",
    "        \n",
    "        # Determine the significance level\n",
    "        if p_value < 0.001:\n",
    "            significance = '***'\n",
    "        elif p_value < 0.01:\n",
    "            significance = '**'\n",
    "        elif p_value < 0.05:\n",
    "            significance = '*'\n",
    "        else:\n",
    "            significance = None\n",
    "\n",
    "        if significance:\n",
    "            significant_regressors.append((regressor, regressor_data.max(), significance))\n",
    "\n",
    "    # Plot the swarmplot\n",
    "    sns.swarmplot(\n",
    "        data=group, \n",
    "        x='regressors', \n",
    "        y='weights', \n",
    "        palette='tab20', \n",
    "        ax=ax, \n",
    "        hue='regressors', \n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(f'Mouse {mouse}')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.hlines(0, -0.5, len(group['regressors'].unique()) - 0.5, color='black', linestyle='--')\n",
    "\n",
    "    # Annotate significant results\n",
    "    for regressor, max_value, significance in significant_regressors:\n",
    "        x = list(group['regressors'].unique()).index(regressor)\n",
    "        y = max_value + 0.05  # Position above max value\n",
    "        ax.text(x, y, significance, ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "# Manually create the legend\n",
    "handles = []\n",
    "for regressor, color in zip(weights_df['regressors'].unique(), sns.color_palette('tab20', len(weights_df['regressors'].unique()))):\n",
    "    handles.append(mpatches.Patch(color=color, label=regressor))\n",
    "\n",
    "# Add legend at the bottom with 3 columns\n",
    "fig.legend(\n",
    "    handles=handles,\n",
    "    bbox_to_anchor=(0.5, 0.05),  # Centered below the figure\n",
    "    loc='upper center',\n",
    "    ncol=3,  # Number of columns\n",
    "    title='Features',\n",
    "    prop={'size': 12}\n",
    ")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.10)  # Add space at the bottom for the legend\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(results_path, f'weights_per_mouse_big_model_{epoch}.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the average weights per regressor\n",
    "# Aggregate the weights by mouse and regressor\n",
    "aggregated_df = weights_df.groupby(['mouse', 'regressors'], as_index=False).mean()\n",
    "\n",
    "# Perform t-tests on the aggregated data\n",
    "t_test_results = []\n",
    "for regressor in aggregated_df['regressors'].unique():\n",
    "    regressor_data = aggregated_df[aggregated_df['regressors'] == regressor]['weights']\n",
    "    t_stat, p_value = ttest_1samp(regressor_data, 0)\n",
    "    \n",
    "    # Determine the significance level\n",
    "    if p_value < 0.001:\n",
    "        significance = '***'\n",
    "    elif p_value < 0.01:\n",
    "        significance = '**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = '*'\n",
    "    else:\n",
    "        significance = None\n",
    "\n",
    "    t_test_results.append({'regressor': regressor, 'p_value': p_value, 'significance': significance})\n",
    "\n",
    "t_test_results_df = pd.DataFrame(t_test_results)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# One point per mouse\n",
    "sns.swarmplot(\n",
    "    data=aggregated_df, \n",
    "    x='regressors', \n",
    "    y='weights', \n",
    "    hue='regressors', \n",
    "    palette='tab20', \n",
    "    dodge=True\n",
    ")\n",
    "\n",
    "# Annotate significance levels\n",
    "for i, row in t_test_results_df.iterrows():\n",
    "    regressor = row['regressor']\n",
    "    significance = row['significance']\n",
    "    if significance:\n",
    "        x = list(aggregated_df['regressors'].unique()).index(regressor)\n",
    "        y = aggregated_df[aggregated_df['regressors'] == regressor]['weights'].max() + 0.1\n",
    "        plt.text(x-0.2, y, significance, ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "# Add horizontal line at 0\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "\n",
    "# Customize labels and legend\n",
    "plt.xlabel('')\n",
    "plt.xlim(-1.5, len(aggregated_df['regressors'].unique()) - 0.5)\n",
    "plt.ylabel('Weight')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Weights Per Regressor (Aggregated by Mouse)')\n",
    "\n",
    "# Manually create legend\n",
    "handles = []\n",
    "for regressor, color in zip(aggregated_df['regressors'].unique(), sns.color_palette('tab20', len(aggregated_df['regressors'].unique()))):\n",
    "    handles.append(mpatches.Patch(color=color, label=regressor))\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.3)  # Adjust space for the legend\n",
    "sns.despine()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(results_path, f'weights_all_big_model_{epoch}.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Remove variables one at a time to find which ones are needed for the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=summary_df.loc[summary_df.engaged == True], x='active_patch', y='has_choice', errorbar=None, palette='tab20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = load()\n",
    "epoch = 'control'\n",
    "summary_df = summary_df.loc[(summary_df.experiment == epoch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the logistic regression model removing one feature at a time\n",
    "weights_df = pd.DataFrame(columns=features)\n",
    "cv_results_df = pd.DataFrame(columns=['mouse', 'cv_score', 'feature_removed'])\n",
    "\n",
    "# Initialize logistic regression model\n",
    "log_reg = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# Option to include polynomial interaction features\n",
    "use_poly = False  # Set to False to disable polynomial features\n",
    "\n",
    "if use_poly:\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    interaction_features = poly.fit_transform(summary_df[features])\n",
    "    interaction_feature_names = poly.get_feature_names_out(features)\n",
    "    all_features = features + list(interaction_feature_names)\n",
    "else:\n",
    "    all_features = features\n",
    "\n",
    "# Iterate over all mice and sessions first\n",
    "for mouse in summary_df['mouse'].unique():\n",
    "    for session in summary_df.loc[summary_df['mouse'] == mouse].session.unique():\n",
    "        mouse_df = summary_df[(summary_df['mouse'] == mouse) & (summary_df['session'] == session)].copy()\n",
    "        \n",
    "        if use_poly:\n",
    "            interaction_features = poly.fit_transform(mouse_df[features])\n",
    "            interaction_df = pd.DataFrame(interaction_features, columns=interaction_feature_names)\n",
    "            X_mouse_full = interaction_df[all_features]\n",
    "        else:\n",
    "            X_mouse_full = mouse_df[all_features]\n",
    "        \n",
    "        y_mouse = mouse_df['has_choice'].astype(int)\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('log_reg', log_reg)\n",
    "        ])\n",
    "        \n",
    "        if len(y_mouse) < 20:\n",
    "            continue\n",
    "        \n",
    "        # First pass with all features\n",
    "        cv_scores = cross_val_score(pipeline, X_mouse_full, y_mouse, cv=5, scoring='roc_auc')\n",
    "        cv_results_df = pd.concat([cv_results_df, pd.DataFrame({\n",
    "            'feature_removed': ['baseline'],\n",
    "            'session': [session],\n",
    "            'mouse': [mouse],\n",
    "            'cv_score': [cv_scores.mean()]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        # Iterate over features to remove one at a time\n",
    "        for feature in all_features:\n",
    "            features_to_use = [f for f in all_features if f != feature]\n",
    "            print(f\"Mouse: {mouse}, Session: {session}, Removing feature: {feature}\")\n",
    "            \n",
    "            X_mouse = interaction_df[features_to_use] if use_poly else mouse_df[features_to_use]\n",
    "            \n",
    "            cv_scores = cross_val_score(pipeline, X_mouse, y_mouse, cv=5)\n",
    "            \n",
    "            cv_results_df = pd.concat([cv_results_df, pd.DataFrame({\n",
    "                'feature_removed': [feature],\n",
    "                'session': [session],\n",
    "                'mouse': [mouse],\n",
    "                'cv_score': [cv_scores.mean()]\n",
    "            })], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.boxplot(data=cv_results_df, x='mouse', y='cv_score', hue='feature_removed', palette='tab10')\n",
    "plt.xticks(rotation=45)\n",
    "sns.despine()\n",
    "plt.legend(title='Feature removed', loc='upper left', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "sns.boxplot(data=cv_results_df, x='feature_removed', y='cv_score', hue='feature_removed', palette='tab10')\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "sns.despine()\n",
    "plt.legend(title='Feature removed', loc='upper left', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many features should I use for the model, how many are useful?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target\n",
    "X = summary_df[features]\n",
    "y = summary_df['has_choice'].astype(int)\n",
    "\n",
    "cv_scores = []  # Store the cross-validation scores\n",
    "# Initialize the logistic regression model\n",
    "log_reg = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# Loop through different numbers of features to select\n",
    "for num_features in range(1, len(features) + 1):\n",
    "    rfe = RFE(log_reg, n_features_to_select=num_features)\n",
    "    X_rfe = rfe.fit_transform(X, y)  # Apply RFE\n",
    "    cv_score = cross_val_score(log_reg, X_rfe, y, cv=5, scoring='roc_auc').mean()  # Calculate cross-validation score\n",
    "    cv_scores.append(cv_score)\n",
    "    # Get the ranking of features (1 means the feature is selected)\n",
    "    selected_features = [features[i] for i in range(len(features)) if rfe.support_[i]]\n",
    "    print(f\"Number of features: {num_features}, Selected features: {selected_features}, Cross-validation score: {cv_score:.2f}\")\n",
    "    \n",
    "# Find the number of features that gives the highest cross-validation score\n",
    "optimal_num_features = np.argmax(cv_scores) + 1  # Adding 1 because range starts from 1\n",
    "print(f\"Optimal number of features: {optimal_num_features}\")\n",
    "\n",
    "# Plot the cross-validation scores for different numbers of features\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(features) + 1), cv_scores, marker='o')\n",
    "plt.title('Cross-validation Scores vs. Number of Features')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cross-validation Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = summary_df[features].corr()\n",
    "\n",
    "# Identify highly correlated features (threshold = 0.9 for example)\n",
    "high_corr = [(i, j) for i in corr_matrix.columns for j in corr_matrix.columns if corr_matrix.loc[i, j] > 0.9 and i != j]\n",
    "print(\"Highly correlated features:\", high_corr)\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, center=0)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fit simulated data with different strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'simulation_data_separate_odors.csv'\n",
    "# filename = 'simulation_data_separate_odors.csv'\n",
    "\n",
    "simulation_df = pd.read_csv(os.path.join(data_path, filename), index_col=0)\n",
    "\n",
    "simulation_df.rename(columns={'rewards_in_patch': 'cumulative_rewards',\n",
    "                              'time_in_patch':'visit_number',\n",
    "                              'failures_in_patch': 'cumulative_failures',\n",
    "                              'patch_id': 'odor_label',\n",
    "                              'patch_entry_time': 'active_patch',\n",
    "                              'prob_reward': 'reward_probability',\n",
    "                              'session_no':'session'}, inplace=True)\n",
    "simulation_df['mouse'] = 'simulation'\n",
    "\n",
    "simulation_df['active_patch'].interpolate(method='linear', inplace=True)\n",
    "# Assign new values when 'values' changes, but restart when 'group' changes\n",
    "simulation_df['active_patch'] = simulation_df.groupby('session')['active_patch'].apply(\n",
    "    lambda x: x.ne(x.shift()).cumsum() - 1  # Detect changes and assign numbers\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# simulation_df['visit_number'] = np.where(simulation_df['odor_label'] == -1, 1, simulation_df['visit_number'])\n",
    "simulation_df['shift_has_choice'] = np.where(simulation_df['odor_label'] == -1, 0, 1)\n",
    "simulation_df['has_choice'] = simulation_df['shift_has_choice'].shift(-1)\n",
    "simulation_df  = simulation_df.loc[simulation_df['odor_label'] != -1]\n",
    "simulation_df['has_choice'] = simulation_df['has_choice'].fillna(0)\n",
    "# simulation_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_weights_df = pd.DataFrame()\n",
    "for strategy in simulation_df['strategy'].unique():\n",
    "    print(f\"Strategy: {strategy}\")\n",
    "    weights_df, cv_results_df, metrics_df, new_mouse_df = logistic_session(simulation_df.loc[simulation_df.strategy == strategy], \n",
    "                                                                        use_polynomial_features=False, \n",
    "                                                                        orig_features=features)\n",
    "    weights_df['strategy'] = strategy\n",
    "    cum_weights_df = pd.concat([cum_weights_df, weights_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot animal per animal weights of the coeficients\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6), sharex=True)\n",
    "\n",
    "# Perform t-tests and plot significance\n",
    "for (mouse, group), ax in zip(cum_weights_df.groupby('strategy'), axes.flatten()):\n",
    "    # Perform t-test for each regressor in the group\n",
    "    significant_regressors = []\n",
    "    for regressor in group['regressors'].unique():\n",
    "        regressor_data = group[group['regressors'] == regressor]['weights']\n",
    "        t_stat, p_value = ttest_1samp(regressor_data, 0)\n",
    "        \n",
    "        # Determine the significance level\n",
    "        if p_value < 0.001:\n",
    "            significance = '***'\n",
    "        elif p_value < 0.01:\n",
    "            significance = '**'\n",
    "        elif p_value < 0.05:\n",
    "            significance = '*'\n",
    "        else:\n",
    "            significance = None\n",
    "\n",
    "        if significance:\n",
    "            significant_regressors.append((regressor, regressor_data.max(), significance))\n",
    "\n",
    "    # Plot the swarmplot\n",
    "    sns.swarmplot(\n",
    "        data=group, \n",
    "        x='regressors', \n",
    "        y='weights', \n",
    "        palette='tab10', \n",
    "        ax=ax, \n",
    "        hue='regressors', \n",
    "        legend=False, \n",
    "        order=['active_patch', 'consecutive_failures', 'cumulative_rewards', 'reward_probability', 'visit_number']\n",
    "    )\n",
    "    ax.set_title(f'Mouse {mouse}')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.hlines(0, -0.5, len(group['regressors'].unique()) - 0.5, color='black', linestyle='--')\n",
    "\n",
    "    # # Annotate significant results\n",
    "    # for regressor, max_value, significance in significant_regressors:\n",
    "    #     x = list(group['regressors'].unique()).index(regressor)\n",
    "    #     y = max_value + 0.05  # Position above max value\n",
    "    #     ax.text(x, y, significance, ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "# Manually create the legend\n",
    "handles = []\n",
    "for regressor, color in zip(cum_weights_df['regressors'].unique(), sns.color_palette('tab10', len(cum_weights_df['regressors'].unique()))):\n",
    "    handles.append(mpatches.Patch(color=color, label=regressor))\n",
    "\n",
    "# Add legend at the bottom with 3 columns\n",
    "fig.legend(\n",
    "    handles=handles,\n",
    "    bbox_to_anchor=(0.6, 0.05),  # Centered below the figure\n",
    "    loc='upper center',\n",
    "    ncol=3,  # Number of columns\n",
    "    title='Features',\n",
    "    prop={'size': 12}\n",
    ")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust()  # Add space at the bottom for the legend\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(results_path, f'weights_per_mouse_small_model_{epoch}.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Running AIC and BIC for model comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.stats import norm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_session(summary_df, \n",
    "                              use_polynomial_features=True, \n",
    "                              orig_features = ['reward_probability', 'consecutive_failures', 'visit_number', 'cumulative_rewards', 'active_patch']):\n",
    "\n",
    "    # Initialize dataframes to store weights and cross-validation results\n",
    "    weights_df = pd.DataFrame(columns=['regressors', 'weights', 'mouse', 'session'])\n",
    "    cv_results_df = pd.DataFrame()\n",
    "    metrics_list = []\n",
    "    new_mouse_df = pd.DataFrame()\n",
    "    \n",
    "    for (mouse, session), mouse_df in summary_df.groupby(['mouse', 'session']):\n",
    "        print(f\"Mouse: {mouse}, Session: {session}\")\n",
    "        \n",
    "        # Select features and target variable\n",
    "        X_mouse = mouse_df[orig_features]\n",
    "        y_mouse = mouse_df['has_choice'].astype(int)\n",
    "                \n",
    "        # Define the pipeline\n",
    "        if use_polynomial_features:\n",
    "            poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "            X_mouse = poly.fit_transform(X_mouse)\n",
    "            features = poly.get_feature_names_out()\n",
    "        else:\n",
    "            features = orig_features\n",
    "        \n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_mouse_scaled = scaler.fit_transform(X_mouse)\n",
    "        \n",
    "        # Perform 5-fold cross-validation\n",
    "        if len(X_mouse_scaled) < 20:\n",
    "            continue\n",
    "        \n",
    "        if y_mouse.nunique() == 1:\n",
    "            continue   \n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True)  # random_state ensures reproducibility\n",
    "        log_reg = LogisticRegression(class_weight='balanced')\n",
    "        cv_scores = cross_val_score(log_reg, X_mouse_scaled, y_mouse, cv=cv, scoring='roc_auc')\n",
    "\n",
    "        # Fit the logistic regression model using formula\n",
    "        log_reg.fit(X_mouse_scaled, y_mouse)\n",
    "\n",
    "        # Predict class labels (0 or 1)\n",
    "        y_pred = log_reg.predict(X_mouse_scaled)\n",
    "        mouse_df['y_pred'] = y_pred\n",
    "        \n",
    "        y_probs = log_reg.predict_proba(X_mouse_scaled)[:, 1]\n",
    "        mouse_df['y_pred_prob'] = y_probs\n",
    "        \n",
    "        best_threshold = plotting_roc_curve(y_probs, y_mouse)\n",
    "        \n",
    "        y_pred_adjusted = (y_probs >= best_threshold).astype(int)\n",
    "        mouse_df['y_pred_adjusted'] = y_pred_adjusted\n",
    "        \n",
    "        # Log-likelihood\n",
    "        log_likelihood = -log_loss(y_mouse, y_probs, normalize=False)\n",
    "\n",
    "        # Number of parameters (coefficients + intercept)\n",
    "        k = len(log_reg.coef_[0]) + 1  # coef_ has shape (1, n_features), so adding 1 for the intercept\n",
    "\n",
    "        # Calculate AIC\n",
    "        aic = 2 * k - 2 * log_likelihood\n",
    "\n",
    "        # Coefficients and p-values\n",
    "        coef = log_reg.coef_[0]\n",
    "\n",
    "        # Compute covariance matrix (approximated)\n",
    "        cov_matrix = np.linalg.inv(np.dot(X_mouse_scaled.T, X_mouse_scaled))  # This is a rough approximation\n",
    "        stderr = np.sqrt(np.diag(cov_matrix))\n",
    "\n",
    "        # Calculate p-values for each coefficient\n",
    "        p_values = 2 * (1 - norm.cdf(np.abs(coef / stderr)))\n",
    "\n",
    "        # Compute class weights like sklearn does\n",
    "        class_counts = Counter(y_mouse)  # Count occurrences of each class\n",
    "        total_samples = len(y_mouse)\n",
    "        num_classes = len(class_counts)\n",
    "\n",
    "        # Compute weight for each class\n",
    "        class_weight = {cls: total_samples / (num_classes * count) for cls, count in class_counts.items()}\n",
    "\n",
    "        # Assign sample weights based on class\n",
    "        sample_weights = y_mouse.map(class_weight)\n",
    "\n",
    "        # Fit statsmodels logistic regression with sample weights\n",
    "        X_sm = sm.add_constant(X_mouse_scaled)  # Add intercept\n",
    "        model_sm = sm.Logit(y_mouse, X_sm)\n",
    "        result_sm = model_sm.fit(weights=sample_weights, disp=0)  # Use weights\n",
    "        \n",
    "        # Get AIC\n",
    "        aic_sm_balanced = result_sm.aic\n",
    "\n",
    "        # Display AIC and coefficients/p-values\n",
    "        print(f\"AIC: {aic}\")\n",
    "        print(f\"AIC (statsmodels): {aic_sm_balanced}\")\n",
    "\n",
    "        features_sklearn = ['Intercept'] + list(features)  # Match feature names\n",
    "        coef_sklearn = np.concatenate(([log_reg.intercept_[0]], log_reg.coef_[0]))  # Include intercept\n",
    "        coef_statsmodels = result_sm.params.values  # Includes intercept\n",
    "\n",
    "        # --- Compare Coefficients ---\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': features_sklearn,\n",
    "            'Coef_sklearn': coef_sklearn,\n",
    "            'Coef_statsmodels': coef_statsmodels\n",
    "        })\n",
    "        \n",
    "        metrics_list = calculate_metrics(metrics_list, y_mouse, y_pred_adjusted)\n",
    "        \n",
    "        feature_weights = pd.Series(log_reg.coef_[0], index=features)\n",
    "        feature_weights = feature_weights.reset_index()\n",
    "        \n",
    "        feature_weights.rename(columns={'index': 'regressors', 0: 'weights'}, inplace=True)\n",
    "        feature_weights['p_values'] = p_values\n",
    "        feature_weights['mouse'] = mouse\n",
    "        feature_weights['session'] = session\n",
    "\n",
    "        # Append the weights and cv scores to the respective dataframes\n",
    "        weights_df = pd.concat([weights_df, feature_weights], ignore_index=True)\n",
    "        cv_results_df = pd.concat([cv_results_df, pd.DataFrame({'session': [session], 'mouse': [mouse], 'cv_std': [cv_scores.std()],\n",
    "                                                                'cv_score': [cv_scores.mean()]})], ignore_index=True)\n",
    "        \n",
    "        new_mouse_df = pd.concat([new_mouse_df, mouse_df], ignore_index=True)\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "    weights_df['mouse'] = weights_df['mouse'].round(0).astype(str)\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    return weights_df, cv_results_df, metrics_df, new_mouse_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df, cv_results_df, metrics_df, new_mouse_df = logistic_session(summary_df, \n",
    "                              use_polynomial_features=False, \n",
    "                              orig_features = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fit different types of sessions and return the difference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_weights_df = pd.DataFrame(columns=['regressors', 'weights', 'experiment'])\n",
    "cum_cv_results_df = pd.DataFrame(columns=['experiment', 'cv_score'])\n",
    "\n",
    "for experiment in summary_df['experiment'].unique():\n",
    "    print(f\"Experiment: {experiment}\")\n",
    "    experiment_df = summary_df[(summary_df['experiment'] == experiment)&(summary_df.label == 'RewardSite')]\n",
    "    weights_df, cv_results_df, metrics_df, new_mouse_df = logistic_session(experiment_df, \n",
    "                              use_polynomial_features=False)\n",
    "    weights_df['experiment'] = experiment\n",
    "    cv_results_df['experiment'] = experiment\n",
    "    cum_cv_results_df = pd.concat([cum_cv_results_df, cv_results_df], ignore_index=True)\n",
    "    cum_weights_df = pd.concat([cum_weights_df, weights_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CV ROC across all the experiments in an histogram\n",
    "for experiment in cum_cv_results_df.experiment.unique():\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    sns.histplot(data=cum_cv_results_df.loc[cum_cv_results_df.experiment == experiment], x='cv_score',  multiple=\"stack\", bins=30, hue='experiment', ax=ax[0], stat = 'probability', legend=False)\n",
    "    sns.histplot(data=cum_cv_results_df.loc[cum_cv_results_df.experiment == experiment], x='cv_std',  multiple=\"stack\",bins=30, hue='experiment', stat = 'probability',  ax=ax[1])\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WEights for each mouse compared across experiments\n",
    "with PdfPages (os.path.join(results_path, 'across_experiments_big_model.pdf')) as pdf:\n",
    "    for mouse in cum_weights_df['mouse'].unique():\n",
    "        fig, ax = plt.subplots(figsize=(16, 6))\n",
    "        sns.boxplot(data=cum_weights_df.loc[cum_weights_df.mouse == mouse], x='regressors', y='weights', hue='experiment')\n",
    "        plt.title(f'Mouse: {mouse}')\n",
    "        plt.xlabel('')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Experiment', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.hlines(0, -0.5, len(cum_weights_df['regressors'].unique()) - 0.5, color='black', linestyle='--')\n",
    "        sns.despine()\n",
    "        pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WEights for all experiments averaged across mice\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "results_df = cum_weights_df.groupby(['mouse','regressors', 'experiment'], as_index=False).weights.mean()\n",
    "sns.boxplot(data=results_df, x='regressors', y='weights', hue='experiment')\n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Experiment', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.hlines(0, -0.5, len(results_df['regressors'].unique()) - 0.5, color='black', linestyle='--')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function utilities to plot box plots with lines joining each animal\n",
    "\n",
    "from scipy.stats import ttest_rel, ttest_ind\n",
    "def plot_lines(data: pd.DataFrame, ax, variable = 'total_rewards', condition =  'mouse'):\n",
    "    for value in data[condition].unique():\n",
    "        y = data.loc[(data[condition] == value)][variable].values\n",
    "        x = data.loc[(data[condition] == value)].experiment.values\n",
    "        ax.plot(x, y, marker='', linestyle='-', color='black', alpha=0.4, linewidth=1)\n",
    "\n",
    "def plot_significance(general_df: pd.DataFrame, axes, \n",
    "                      variable = 'total_rewards', \n",
    "                      experiment = 'distance_short'):\n",
    "        # Perform statistical test and add significance annotations\n",
    "    group1 = general_df.loc[general_df.experiment == 'control', variable]\n",
    "    group2 = general_df.loc[general_df.experiment == experiment, variable]\n",
    "    # Perform t-test\n",
    "    try:\n",
    "        t_stat, p_value = ttest_rel(group1, group2)\n",
    "    except:\n",
    "        print('Error in t-test paired, running independent t-test')\n",
    "        t_stat, p_value = ttest_ind(group1, group2)\n",
    "    \n",
    "    print(f'{variable} p-value: {p_value}')\n",
    "    # Add significance annotation\n",
    "    x1, x2 = 0, 1  # x-coordinates of the groups\n",
    "    y, h, col = general_df[variable].max() + 1, 0.5, 'k'  # y-coord, line height, color\n",
    "    if variable == 'reward_probability':\n",
    "        y = 0.6\n",
    "        h=0.05\n",
    "        \n",
    "    if p_value < 0.001:\n",
    "        significance = \"***\" \n",
    "    elif p_value < 0.01:\n",
    "        significance = \"**\" \n",
    "    elif p_value < 0.05:\n",
    "        significance = \"*\"\n",
    "    else:\n",
    "        significance = \"ns\"\n",
    "    \n",
    "    axes.plot([x1, x1, x2, x2], [y, y + h, y + h, y], lw=1.5, c=col)\n",
    "    axes.text((x1 + x2) * 0.5, y + h, significance, ha='center', va='bottom', color=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in ['distance_short', 'distance_long', 'distance_extra_short', 'distance_extra_long', 'friction_low', 'friction_med', 'friction_high']:\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "    for regressor, ax in zip(cum_weights_df['regressors'].unique(), axes.flatten()):\n",
    "        general_df = cum_weights_df.loc[(cum_weights_df.experiment == 'control')|(cum_weights_df.experiment == experiment)].groupby(['mouse', 'experiment', 'regressors'], as_index=False).weights.mean()\n",
    "        general_df = general_df.loc[general_df.regressors == regressor]\n",
    "        \n",
    "        # Keep only mice that have both experiment types\n",
    "        mice_with_both = (\n",
    "            general_df.groupby(\"mouse\")[\"experiment\"]\n",
    "            .nunique()\n",
    "            .eq(2)  # Ensures the mouse has both 'control' and 'distance_short'\n",
    "        )\n",
    "\n",
    "        # Filter the DataFrame to keep only those mice\n",
    "        general_df = general_df[general_df[\"mouse\"].isin(mice_with_both[mice_with_both].index)]\n",
    "        \n",
    "        sns.boxplot(x='experiment', y='weights', data=general_df, hue='experiment', legend=False, width=0.5, ax=ax)\n",
    "        ax.hlines(0, -0.5, 1.5, color='black', linestyle='--')\n",
    "        ax.set_xlabel(regressor)\n",
    "        plot_lines(general_df, ax, 'weights', 'mouse')\n",
    "        plot_significance(general_df, ax, 'weights', experiment=experiment)\n",
    "        plt.suptitle(experiment)\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Crossvalidate num of parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate interaction terms and keep track of their names\n",
    "def generate_interactions(df, features):\n",
    "    interactions = []\n",
    "    interaction_names = []  # List to store names of interactions\n",
    "    for feature1, feature2 in itertools.combinations(features, 2):\n",
    "        interaction_name = f'{feature1}*{feature2}'  # Interaction term name\n",
    "        interaction_names.append(interaction_name)\n",
    "        interactions.append(df[feature1] * df[feature2])  # Create the interaction term\n",
    "    return interactions, interaction_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate_feature_selection_iteration(summary_df):\n",
    "    \n",
    "    # Define the features and target\n",
    "    features = ['reward_probability', 'consecutive_failures', 'visit_number', 'cumulative_rewards', 'active_patch']\n",
    "    X = summary_df[features]\n",
    "    y = summary_df['has_choice'].astype(int)\n",
    "\n",
    "    # Generate interaction terms and their names\n",
    "    interaction_terms, interaction_names = generate_interactions(X, features)\n",
    "\n",
    "    # Add the interaction terms to the feature set\n",
    "    X_with_interactions = pd.concat([X] + [pd.Series(interaction_terms[i], name=interaction_names[i]) for i in range(len(interaction_terms))], axis=1)\n",
    "\n",
    "    # Initialize the logistic regression model\n",
    "    log_reg = LogisticRegression(class_weight='balanced', C=1)\n",
    "\n",
    "    # List to store the cross-validation scores and selected features for each number of features selected\n",
    "    cv_scores = []\n",
    "    selected_features_list = []\n",
    "\n",
    "    # Loop through different numbers of features to select\n",
    "    for num_features in range(1, len(X_with_interactions.columns) + 1):\n",
    "        rfe = RFE(log_reg, n_features_to_select=num_features)\n",
    "        rfe.fit(X_with_interactions, y)  # Apply RFE\n",
    "        \n",
    "        # Get the selected features based on RFE support_\n",
    "        selected_features = X_with_interactions.columns[rfe.support_]\n",
    "        \n",
    "        # Separate the interaction terms and non-interaction features\n",
    "        selected_interactions = [name for name in selected_features if '*' in name]  # Interaction names contain '*'\n",
    "        selected_non_interactions = [name for name in selected_features if '*' not in name]  # Non-interaction names\n",
    "        \n",
    "        selected_features_list.append((selected_interactions, selected_non_interactions))\n",
    "        \n",
    "        # Calculate the cross-validation score for the selected features\n",
    "        X_rfe = rfe.transform(X_with_interactions)  # Apply RFE transformation\n",
    "        cv_score = cross_val_score(log_reg, X_rfe, y, cv=5, scoring='roc_auc').mean()  # Calculate cross-validation score\n",
    "        cv_scores.append(cv_score)\n",
    "\n",
    "        # Print the selected interaction and non-interaction features at this iteration\n",
    "        print(f\"Selected interaction terms for {num_features} features: {selected_interactions}, {selected_non_interactions}\")\n",
    "\n",
    "    # Find the number of features that gives the highest cross-validation score\n",
    "    optimal_num_features = np.argmax(cv_scores) + 1  # Adding 1 because range starts from 1\n",
    "    print(f\"Optimal number of features (with interactions): {optimal_num_features}\")\n",
    "    \n",
    "    return cv_scores, selected_features_list, optimal_num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def crossvalidate_feature_selection(summary_df):\n",
    "    # Define the features and target\n",
    "    features = ['reward_probability', 'consecutive_failures', 'visit_number', 'cumulative_rewards', 'active_patch']\n",
    "    X = summary_df[features]\n",
    "    y = summary_df['has_choice'].astype(int)\n",
    "\n",
    "    # Generate interaction terms and their names\n",
    "    interaction_terms, interaction_names = generate_interactions(X, features)\n",
    "\n",
    "    # Add the interaction terms to the feature set\n",
    "    X_with_interactions = pd.concat([X] + [pd.Series(interaction_terms[i], name=interaction_names[i]) for i in range(len(interaction_terms))], axis=1)\n",
    "\n",
    "    # Initialize the logistic regression model\n",
    "    log_reg = LogisticRegression(class_weight='balanced', C=1)\n",
    "\n",
    "    rfecv = RFECV(estimator=log_reg, cv=StratifiedKFold(5), scoring='roc_auc')\n",
    "    rfecv.fit(X_with_interactions, y)\n",
    "    # Optimal number of features\n",
    "    print(\"Optimal number of features:\", rfecv.n_features_)\n",
    "\n",
    "    # Selected features\n",
    "    print(\"Selected Features:\", X_with_interactions.columns[rfecv.support_])\n",
    "\n",
    "    # Model with all features\n",
    "    log_reg.fit(X_with_interactions, y)\n",
    "    all_features_pred = log_reg.predict(X_with_interactions)\n",
    "    print(\"Accuracy with all features:\", accuracy_score(y, all_features_pred))\n",
    "\n",
    "    # Model with selected features\n",
    "    X_selected = X_with_interactions.loc[:, rfecv.support_]\n",
    "    log_reg.fit(X_selected, y)\n",
    "    selected_features_pred = log_reg.predict(X_selected)\n",
    "    print(\"Accuracy with selected features:\", accuracy_score(y, selected_features_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for experiment in summary_df['experiment'].unique():\n",
    "    print(f\"Experiment: {experiment}\")\n",
    "    experiment_df = summary_df[(summary_df['experiment'] == experiment)&(summary_df.label == 'RewardSite')]\n",
    "    \n",
    "    cv_scores, selected_features_list, optimal_num_features = crossvalidate_feature_selection_iteration(experiment_df)\n",
    "\n",
    "    # Plot the cross-validation scores for different numbers of features\n",
    "    plt.plot(range(1, len(X_with_interactions.columns) + 1), cv_scores, marker='o', label=experiment)\n",
    "    plt.plot(optimal_num_features, max(cv_scores), 'ro')  # Highlight the optimal number of features\n",
    "    \n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title('Cross-validation Scores vs. Number of Features (with Interactions)')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cross-validation Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in summary_df['experiment'].unique():\n",
    "    print(f\"Experiment: {experiment}\")\n",
    "    experiment_df = summary_df[(summary_df['experiment'] == experiment)&(summary_df.label == 'RewardSite')]\n",
    "    \n",
    "    crossvalidate_feature_selection(experiment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GLM for median split of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = load()\n",
    "epoch = 'control'\n",
    "summary_df = summary_df.loc[(summary_df.experiment == epoch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the patch number per session and animal\n",
    "summary_df['norm_patch_number'] = summary_df.groupby(['mouse', 'session'])['active_patch'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_weights_df = pd.DataFrame(columns=['regressors', 'weights'])\n",
    "cum_cv_results_df = pd.DataFrame(columns=['experiment', 'cv_score'])\n",
    "\n",
    "features = ['reward_probability', 'consecutive_failures', 'visit_number', 'cumulative_rewards', \n",
    "                                     'active_patch']\n",
    "\n",
    "for half_df, name in zip([summary_df.loc[summary_df['norm_patch_number'] < 0.3], summary_df.loc[summary_df['norm_patch_number'] > 0.3]], ['first', 'second']):\n",
    "    weights_df, cv_results_df, metrics_df, new_mouse_df = logistic_session(half_df, \n",
    "                                use_polynomial_features=False, orig_features=features)\n",
    "\n",
    "    weights_df['half'] = name\n",
    "    cum_weights_df = pd.concat([cum_weights_df, weights_df], ignore_index=True)\n",
    "    cum_cv_results_df = pd.concat([cum_cv_results_df, cv_results_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the average weights per regressor for both halves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "for ax, half in zip(axes, ['first', 'second']):\n",
    "    # Aggregate the weights by mouse and regressor\n",
    "    aggregated_df = cum_weights_df.loc[cum_weights_df.half == half].groupby(['mouse', 'regressors'], as_index=False)['weights'].mean()\n",
    "\n",
    "    # Perform t-tests on the aggregated data\n",
    "    t_test_results = []\n",
    "    for regressor in aggregated_df['regressors'].unique():\n",
    "        regressor_data = aggregated_df[aggregated_df['regressors'] == regressor]['weights']\n",
    "        t_stat, p_value = ttest_1samp(regressor_data, 0)\n",
    "        \n",
    "        # Determine the significance level\n",
    "        if p_value < 0.001:\n",
    "            significance = '***'\n",
    "        elif p_value < 0.01:\n",
    "            significance = '**'\n",
    "        elif p_value < 0.05:\n",
    "            significance = '*'\n",
    "        else:\n",
    "            significance = None\n",
    "\n",
    "        t_test_results.append({'regressor': regressor, 'p_value': p_value, 'significance': significance})\n",
    "\n",
    "    t_test_results_df = pd.DataFrame(t_test_results)\n",
    "\n",
    "    # Plot\n",
    "    sns.swarmplot(\n",
    "        data=aggregated_df, \n",
    "        x='regressors', \n",
    "        y='weights', \n",
    "        hue='regressors', \n",
    "        palette=palette, \n",
    "        dodge=True,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Annotate significance levels\n",
    "    for i, row in t_test_results_df.iterrows():\n",
    "        regressor = row['regressor']\n",
    "        significance = row['significance']\n",
    "        if significance:\n",
    "            x = list(aggregated_df['regressors'].unique()).index(regressor)\n",
    "            y = aggregated_df[aggregated_df['regressors'] == regressor]['weights'].max() + 0.1\n",
    "            ax.text(x-0.2, y, significance, ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "    # Add horizontal line at 0\n",
    "    ax.axhline(0, color='black', linestyle='--')\n",
    "\n",
    "    # Customize labels and legend\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xlim(-1.5, len(aggregated_df['regressors'].unique()) - 0.5)\n",
    "    ax.set_ylabel('Weight')\n",
    "    ax.set_xticks(range(len(aggregated_df['regressors'].unique())))\n",
    "    ax.set_xticklabels(aggregated_df['regressors'].unique(), rotation=45, ha='right')\n",
    "    ax.set_title(f'{half.capitalize()} Half)')\n",
    "\n",
    "fig.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left', title='Regressors')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.3, right=0.8)  # Adjust space for the legend\n",
    "sns.despine()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(results_path, 'glm_weights_split_session.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fit all sessions of a mouse together, add friction and distance as parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure if this is correct since I am treating all the sessions together without aknowledging it in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_mouse(summary_df, \n",
    "                    use_polynomial_features=True, \n",
    "                    orig_features = ['reward_probability', 'consecutive_failures', 'visit_number', 'cumulative_rewards', \n",
    "                                     'active_patch', 'torque_friction', 'session_n']):\n",
    "\n",
    "    # Initialize dataframes to store weights and cross-validation results\n",
    "    weights_df = pd.DataFrame(columns=['regressors', 'weights', 'mouse', 'session'])\n",
    "    cv_results_df = pd.DataFrame()\n",
    "    metrics_list = []\n",
    "    new_mouse_df = pd.DataFrame()\n",
    "    \n",
    "    for mouse, mouse_df in summary_df.groupby(['mouse']):\n",
    "        print(f\"Mouse: {mouse[0]}\")\n",
    "        \n",
    "        # Select features and target variable\n",
    "        X_mouse = mouse_df[orig_features]\n",
    "        y_mouse = mouse_df['has_choice'].astype(int)\n",
    "        \n",
    "        if 'session_n' in orig_features:\n",
    "            X_mouse = pd.get_dummies(X_mouse, columns=['session_n'], prefix='session_n')\n",
    "        \n",
    "        # Define the pipeline\n",
    "        if use_polynomial_features:\n",
    "            poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "            X_mouse = poly.fit_transform(X_mouse)\n",
    "            features = poly.get_feature_names_out()\n",
    "        else:\n",
    "            features = X_mouse.columns\n",
    "        \n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_mouse_scaled = scaler.fit_transform(X_mouse)\n",
    "        \n",
    "        # Perform 5-fold cross-validation\n",
    "        if len(X_mouse_scaled) < 20:\n",
    "            continue\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True)  # random_state ensures reproducibility\n",
    "        log_reg = LogisticRegression(class_weight='balanced')\n",
    "        cv_scores = cross_val_score(log_reg, X_mouse_scaled, y_mouse, cv=cv, scoring='roc_auc')\n",
    "\n",
    "        # Fit the logistic regression model using formula\n",
    "        log_reg.fit(X_mouse_scaled, y_mouse)\n",
    "\n",
    "        \n",
    "        # Predict class labels (0 or 1)\n",
    "        y_pred = log_reg.predict(X_mouse_scaled)\n",
    "        mouse_df['y_pred'] = y_pred\n",
    "        \n",
    "        y_probs = log_reg.predict_proba(X_mouse_scaled)[:, 1]\n",
    "        mouse_df['y_pred_prob'] = y_probs\n",
    "        \n",
    "        best_threshold = plotting_roc_curve(y_probs, y_mouse)\n",
    "        \n",
    "        y_pred_adjusted = (y_probs >= best_threshold).astype(int)\n",
    "        mouse_df['y_pred_adjusted'] = y_pred_adjusted\n",
    "        \n",
    "        metrics_list = calculate_metrics(metrics_list, y_mouse, y_pred_adjusted)\n",
    "        \n",
    "        feature_weights = pd.Series(log_reg.coef_[0], index=features)\n",
    "        feature_weights = feature_weights.reset_index()\n",
    "        feature_weights.rename(columns={'index': 'regressors', 0: 'weights'}, inplace=True)\n",
    "        feature_weights['mouse'] = mouse[0]\n",
    "\n",
    "        # Append the weights and cv scores to the respective dataframes\n",
    "        weights_df = pd.concat([weights_df, feature_weights], ignore_index=True)\n",
    "        cv_results_df = pd.concat([cv_results_df, pd.DataFrame({'mouse': [mouse], 'cv_std': [cv_scores.std()],\n",
    "                                                                'cv_score': [cv_scores.mean()]})], ignore_index=True)\n",
    "        \n",
    "        new_mouse_df = pd.concat([new_mouse_df, mouse_df], ignore_index=True)\n",
    "\n",
    "    weights_df['mouse'] = weights_df['mouse'].round(0).astype(str)\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return weights_df, cv_results_df, metrics_df, new_mouse_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = load(interpatch_name='PostPatch')\n",
    "summary_df = summary_df.loc[summary_df.experiment != 'data_collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.interpatch_time.fillna(0, inplace=True)\n",
    "summary_df.interpatch_length.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_weights_df = pd.DataFrame(columns=['regressors', 'weights'])\n",
    "cum_cv_results_df = pd.DataFrame(columns=['experiment', 'cv_score'])\n",
    "\n",
    "features = ['reward_probability', 'consecutive_failures', 'visit_number', 'cumulative_rewards', \n",
    "                                     'active_patch', 'torque_friction', 'interpatch_time', 'interpatch_length']\n",
    "\n",
    "weights_df, cv_results_df, metrics_df, new_mouse_df = logistic_mouse(summary_df, \n",
    "                            use_polynomial_features=False, orig_features=features)\n",
    "\n",
    "if 'session_n' in features:\n",
    "    weights_df = weights_df[~weights_df.apply(lambda row: row.astype(str).str.startswith('session_')).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {\n",
    "    'reward_probability': (0.12156862745098039, 0.4666666666666667, 0.7058823529411765),\n",
    "    'consecutive_failures': (1.0, 0.4980392156862745, 0.054901960784313725),\n",
    "    'visit_number': (0.17254901960784313, 0.6274509803921569, 0.17254901960784313),\n",
    "    'cumulative_rewards': (0.8392156862745098, 0.15294117647058825, 0.1568627450980392),\n",
    "    'active_patch': (0.5803921568627451, 0.403921568627451, 0.7411764705882353),\n",
    "    'torque_friction': (0.5490196078431373, 0.33725490196078434, 0.29411764705882354),\n",
    "    'interpatch_time': (0.8901960784313725, 0.4666666666666667, 0.7607843137254902),\n",
    "    'interpatch_length': (0.4980392156862745, 0.4980392156862745, 0.4980392156862745)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the weights by mouse and regressor\n",
    "aggregated_df = weights_df.groupby(['mouse', 'regressors'], as_index=False).weights.mean()\n",
    "\n",
    "# Perform t-tests on the aggregated data\n",
    "t_test_results = []\n",
    "for regressor in aggregated_df['regressors'].unique():\n",
    "    regressor_data = aggregated_df[aggregated_df['regressors'] == regressor]['weights']\n",
    "    t_stat, p_value = ttest_1samp(regressor_data, 0)\n",
    "    \n",
    "    # Determine the significance level\n",
    "    if p_value < 0.001:\n",
    "        significance = '***'\n",
    "    elif p_value < 0.01:\n",
    "        significance = '**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = '*'\n",
    "    else:\n",
    "        significance = None\n",
    "\n",
    "    t_test_results.append({'regressor': regressor, 'p_value': p_value, 'significance': significance})\n",
    "\n",
    "t_test_results_df = pd.DataFrame(t_test_results)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# One point per mouse\n",
    "sns.swarmplot(\n",
    "    data=aggregated_df, \n",
    "    x='regressors', \n",
    "    y='weights', \n",
    "    hue='regressors', \n",
    "    dodge=True\n",
    ")\n",
    "\n",
    "# Annotate significance levels\n",
    "for i, row in t_test_results_df.iterrows():\n",
    "    regressor = row['regressor']\n",
    "    significance = row['significance']\n",
    "    if significance:\n",
    "        x = list(aggregated_df['regressors'].unique()).index(regressor)\n",
    "        y = aggregated_df[aggregated_df['regressors'] == regressor]['weights'].max() + 0.1\n",
    "        plt.text(x, y, significance, ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "# Add horizontal line at 0\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "\n",
    "# Customize labels and legend\n",
    "plt.xlabel('')\n",
    "plt.xlim(-1, len(aggregated_df['regressors'].unique()))\n",
    "plt.ylabel('Weight')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "# plt.xticks([])\n",
    "plt.suptitle('Weights Per Regressor \\n (Aggregated by Mouse)')\n",
    "\n",
    "# Manually create legend\n",
    "handles = []\n",
    "for regressor in aggregated_df['regressors'].unique():\n",
    "    handles.append(mpatches.Patch(color=palette[regressor], label=regressor))\n",
    "plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = summary_df[features].corr()\n",
    "\n",
    "# Identify highly correlated features (threshold = 0.9 for example)\n",
    "high_corr = [(i, j) for i in corr_matrix.columns for j in corr_matrix.columns if corr_matrix.loc[i, j] > 0.9 and i != j]\n",
    "print(\"Highly correlated features:\", high_corr)\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, center=0)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fit odor_labels separately**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = load()\n",
    "features = ['reward_probability', 'consecutive_failures', 'visit_number', 'cumulative_rewards', 'active_patch']\n",
    "\n",
    "epoch = 'control'\n",
    "summary_df = summary_df.loc[(summary_df.experiment == epoch)]\n",
    "summary_df = summary_df.loc[summary_df.visit_number > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_weights_df = pd.DataFrame(columns=['regressors', 'weights'])\n",
    "cum_cv_results_df = pd.DataFrame()\n",
    "\n",
    "features = ['reward_probability', 'consecutive_failures', 'visit_number', 'cumulative_rewards', \n",
    "                                     'active_patch']\n",
    "\n",
    "for odor_label in summary_df['odor_label'].unique():\n",
    "    weights_df, cv_results_df, metrics_df, new_mouse_df = logistic_session(summary_df.loc[summary_df.odor_label == odor_label], \n",
    "                                use_polynomial_features=False, orig_features=features)\n",
    "    weights_df['odor_label'] = odor_label\n",
    "    cv_results_df['odor_label'] = odor_label\n",
    "    \n",
    "    if 'session_n' in features:\n",
    "        weights_df = weights_df[~weights_df.apply(lambda row: row.astype(str).str.startswith('session_')).any(axis=1)]\n",
    "        \n",
    "    cum_weights_df = pd.concat([cum_weights_df, weights_df], ignore_index=True)\n",
    "    cum_cv_results_df = pd.concat([cum_cv_results_df, cv_results_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_cv_results_df.groupby('odor_label').cv_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Separate the data by odor_label\n",
    "alpha_pinene_scores = cv_results_df[cv_results_df['odor_label'] == 'Alpha-pinene']['cv_score'].dropna()\n",
    "methyl_butyrate_scores = cv_results_df[cv_results_df['odor_label'] == 'Methyl Butyrate']['cv_score'].dropna()\n",
    "\n",
    "alpha_pinene_std = cv_results_df[cv_results_df['odor_label'] == 'Alpha-pinene']['cv_std'].dropna()\n",
    "methyl_butyrate_std = cv_results_df[cv_results_df['odor_label'] == 'Methyl Butyrate']['cv_std'].dropna()\n",
    "\n",
    "# Perform the Kolmogorov-Smirnov test\n",
    "ks_score_result = ks_2samp(alpha_pinene_scores, methyl_butyrate_scores)\n",
    "ks_std_result = ks_2samp(alpha_pinene_std, methyl_butyrate_std)\n",
    "\n",
    "print(f\"KS test for cv_score: statistic={ks_score_result.statistic}, p-value={ks_score_result.pvalue}\")\n",
    "print(f\"KS test for cv_std: statistic={ks_std_result.statistic}, p-value={ks_std_result.pvalue}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.suptitle(scoring)\n",
    "sns.histplot(data=cv_results_df, x='cv_score',  bins=30, hue='odor_label', ax=ax[0], legend=False)\n",
    "sns.histplot(data=cv_results_df, x='cv_std',  bins=30, hue='odor_label',  ax=ax[1])\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, axes = plt.subplots(1,2, figsize=(14, 4))\n",
    "\n",
    "for odor_label, ax in zip(['Alpha-pinene', 'Methyl Butyrate'], axes.flatten()):\n",
    "    # Aggregate the weights by mouse and regressor\n",
    "    aggregated_df = cum_weights_df.loc[cum_weights_df.odor_label == odor_label].groupby(['mouse', 'regressors'], as_index=False).weights.mean()\n",
    "\n",
    "    # Perform t-tests on the aggregated data\n",
    "    t_test_results = []\n",
    "    for regressor in aggregated_df['regressors'].unique():\n",
    "        regressor_data = aggregated_df[aggregated_df['regressors'] == regressor]['weights']\n",
    "        t_stat, p_value = ttest_1samp(regressor_data, 0)\n",
    "        \n",
    "        # Determine the significance level\n",
    "        if p_value < 0.001:\n",
    "            significance = '***'\n",
    "        elif p_value < 0.01:\n",
    "            significance = '**'\n",
    "        elif p_value < 0.05:\n",
    "            significance = '*'\n",
    "        else:\n",
    "            significance = None\n",
    "\n",
    "        t_test_results.append({'regressor': regressor, 'p_value': p_value, 'significance': significance})\n",
    "\n",
    "    t_test_results_df = pd.DataFrame(t_test_results)\n",
    "\n",
    "    # One point per mouse\n",
    "    sns.swarmplot(\n",
    "        data=aggregated_df, \n",
    "        x='regressors', \n",
    "        y='weights', \n",
    "        hue='regressors', \n",
    "        palette='tab10', \n",
    "        dodge=True, \n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Annotate significance levels\n",
    "    for i, row in t_test_results_df.iterrows():\n",
    "        regressor = row['regressor']\n",
    "        significance = row['significance']\n",
    "        if significance:\n",
    "            x = list(aggregated_df['regressors'].unique()).index(regressor)\n",
    "            y = aggregated_df[aggregated_df['regressors'] == regressor]['weights'].max() + 0.1\n",
    "            ax.text(x, y, significance, ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "    # Add horizontal line at 0\n",
    "    ax.axhline(0, color='black', linestyle='--')\n",
    "\n",
    "    # Customize labels and legend\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xlim(-1, len(aggregated_df['regressors'].unique()))\n",
    "    ax.set_ylabel('Weight')\n",
    "    ax.set_title(f'Weights {odor_label}')\n",
    "    ax.set_xticklabels([], rotation=45, ha='right')\n",
    "# Manually create legend\n",
    "handles = []\n",
    "for regressor, color in zip(aggregated_df['regressors'].unique(), sns.color_palette('tab10', len(aggregated_df['regressors'].unique()))):\n",
    "    handles.append(mpatches.Patch(color=color, label=regressor))\n",
    "plt.legend(handles=handles, title='Features', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(os.path.join(results_path, f'weights_all_small_model_{epoch}.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using the collapsed data\n",
    "for odor_label in ['Alpha-pinene', 'Methyl Butyrate']:\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(20, 14), sharex=True)\n",
    "\n",
    "    # Perform t-tests and plot significance\n",
    "    for (mouse, group), ax in zip(weights_df.loc[weights_df.odor_label == odor_label].groupby('mouse'), axes.flatten()):\n",
    "        # Perform t-test for each regressor in the group\n",
    "        significant_regressors = []\n",
    "        for regressor in group['regressors'].unique():\n",
    "            regressor_data = group[group['regressors'] == regressor]['weights']\n",
    "            t_stat, p_value = ttest_1samp(regressor_data, 0)\n",
    "            \n",
    "            # Determine the significance level\n",
    "            if p_value < 0.001:\n",
    "                significance = '***'\n",
    "            elif p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif p_value < 0.05:\n",
    "                significance = '*'\n",
    "            else:\n",
    "                significance = None\n",
    "\n",
    "            if significance:\n",
    "                significant_regressors.append((regressor, regressor_data.max(), significance))\n",
    "\n",
    "        # Plot the swarmplot\n",
    "        sns.swarmplot(\n",
    "            data=group, \n",
    "            x='regressors', \n",
    "            y='weights', \n",
    "            palette='tab10', \n",
    "            ax=ax, \n",
    "            hue='regressors',\n",
    "            dodge=True,\n",
    "            legend=False\n",
    "        )\n",
    "        ax.set_title(f'Mouse {mouse}')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.hlines(0, -0.5, len(group['regressors'].unique()) - 0.5, color='black', linestyle='--')\n",
    "\n",
    "        # Annotate significant results\n",
    "        for regressor, max_value, significance in significant_regressors:\n",
    "            x = list(group['regressors'].unique()).index(regressor)\n",
    "            y = max_value + 0.05  # Position above max value\n",
    "            ax.text(x, y, significance, ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "    # Manually create the legend\n",
    "    handles = []\n",
    "    for regressor, color in zip(weights_df['regressors'].unique(), sns.color_palette('tab10', len(weights_df['regressors'].unique()))):\n",
    "        handles.append(mpatches.Patch(color=color, label=regressor))\n",
    "\n",
    "    # Add legend at the bottom with 3 columns\n",
    "    fig.legend(\n",
    "        handles=handles,\n",
    "        bbox_to_anchor=(0.6, 0.05),  # Centered below the figure\n",
    "        loc='upper center',\n",
    "        ncol=3,  # Number of columns\n",
    "        title='Features',\n",
    "        prop={'size': 12}\n",
    "    )\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust()  # Add space at the bottom for the legend\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(results_path, f'weights_per_mouse_small_model_{epoch}_{odor_label}.pdf'), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
