{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "\n",
    "# Third-party library imports\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# Custom library imports\n",
    "from aind_vr_foraging_analysis.utils.parsing import data_access\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c2d1c",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45db737",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This section is to find the data in vast\n",
    "date_string = \"2025-4-2\"\n",
    "mouse = '789918'\n",
    "\n",
    "session_paths = data_access.find_sessions_relative_to_date(\n",
    "    mouse=mouse,\n",
    "    date_string=date_string,\n",
    "    when='on'\n",
    ")\n",
    "\n",
    "for session_path in session_paths:\n",
    "    try:\n",
    "        all_epochs, stream_data, data = data_access.load_session(\n",
    "            session_path\n",
    "        )\n",
    "        reward_sites = all_epochs.loc[all_epochs['label'] == 'OdorSite']\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {session_path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can specify the session path directly if you know it\n",
    "#The following are examples that we used in the past\n",
    "\n",
    "# session_path = Path(r\"C:\\Users\\tiffany.ona\\Downloads\\789918_2025-04-02T203144Z\")\n",
    "# frame_skip = 8  # Skip frames for performance\n",
    "# start_time = 0  # seconds\n",
    "# end_time = 15   # seconds\n",
    "# window = 2      # time window shown on plot in seconds\n",
    "\n",
    "# session_path = Path(r\"C:\\Users\\tiffany.ona\\Downloads\\789924_2025-04-23T183014Z\")\n",
    "# frame_skip = 8  # Skip frames for performance\n",
    "# start_time = 275  # seconds\n",
    "# end_time = 314   # seconds\n",
    "# window = 5      # time window shown on plot in seconds\n",
    "\n",
    "# session_path = Path(r\"C:\\Users\\tiffany.ona\\Downloads\\789907_2025-04-24T181053Z\")\n",
    "# frame_skip = 8  # Skip frames for performance\n",
    "# start_time = 1457  # seconds\n",
    "# end_time = 1483   # seconds\n",
    "# window = 5  \n",
    "\n",
    "# session_path = Path(r\"C:\\Users\\tiffany.ona\\Downloads\\789911_2025-04-24T201010Z\")\n",
    "# frame_skip = 8  # Skip frames for performance\n",
    "# start_time = 3350  # seconds\n",
    "# end_time = 3386   # seconds\n",
    "# window = 5  \n",
    "\n",
    "all_epochs, stream_data, data = data_access.load_session(\n",
    "    session_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be7f218",
   "metadata": {},
   "source": [
    "## Main loop for generating the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d58191",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(session_path, \"behavior-videos\")\n",
    "\n",
    "# === Settings ===\n",
    "camera_view = \"SideCamera\"\n",
    "output_path = f\"synced_video_output_{session_path.name}.mp4\"\n",
    "\n",
    "video_paths = {\n",
    "    cam: os.path.join(video_path, cam, \"video.mp4\") for cam in [\"SideCamera\", \"FrontCamera\", \"FaceCamera\"]\n",
    "}\n",
    "\n",
    "metadata_paths = {\n",
    "    cam: os.path.join(video_path, cam, \"metadata.csv\") for cam in [\"SideCamera\", \"FrontCamera\", \"FaceCamera\"]\n",
    "}\n",
    "\n",
    "# Load metadata\n",
    "try:\n",
    "    sync = pd.read_csv(metadata_paths[camera_view])\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Metadata file not found for {camera_view}.\")\n",
    "    exit(1)\n",
    "    \n",
    "first_time = sync['ReferenceTime'].iloc[0]\n",
    "sync['Frame'] = sync['CameraFrameNumber'] - sync['CameraFrameNumber'].iloc[0]\n",
    "sync.ReferenceTime = sync.ReferenceTime - first_time\n",
    "sync = sync[(sync['ReferenceTime'] >= start_time) & (sync['ReferenceTime'] <= end_time)].reset_index(drop=True)\n",
    "\n",
    "# === Load data ===\n",
    "sniff = stream_data.breathing.copy()\n",
    "speed = stream_data.encoder_data.copy()\n",
    "tone = stream_data.choice_feedback.copy()\n",
    "lick = stream_data.lick_onset.copy()\n",
    "reward = stream_data.give_reward.copy()\n",
    "events_df = all_epochs.copy()\n",
    "\n",
    "# Adjust continuous and discrete data to match sync time\n",
    "sniff.index = sniff.index - first_time\n",
    "mean = np.mean(sniff.data)\n",
    "std = np.std(sniff.data)\n",
    "sniff.data = (sniff.data - mean) / std\n",
    "\n",
    "tone.index = tone.index - first_time\n",
    "lick.index = lick.index - first_time\n",
    "reward.index = reward.index - first_time\n",
    "\n",
    "speed.index = speed.index - first_time\n",
    "\n",
    "events_df.index = events_df.index - first_time\n",
    "events_df['stop_time'] = events_df.stop_time - first_time\n",
    "\n",
    "# Open video captures\n",
    "cap_face = cv2.VideoCapture(video_paths['FaceCamera'])\n",
    "cap_side = cv2.VideoCapture(video_paths['SideCamera'])\n",
    "fps = cap_side.get(cv2.CAP_PROP_FPS) / frame_skip\n",
    "frame_w = int(cap_side.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_h = int(cap_side.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Plot size\n",
    "plot_w, plot_h = 2 * frame_w, frame_h\n",
    "\n",
    "# Set up matplotlib figure\n",
    "fig, (ax3, ax1, ax2) = plt.subplots(3, 1, figsize=(22, 8), gridspec_kw={'height_ratios': [1, 1, 2]}, dpi=100)\n",
    "canvas = FigureCanvas(fig)\n",
    "sniff_line, = ax1.plot([], [], 'k-')\n",
    "speed_line, = ax2.plot([], [], 'k-')\n",
    "ax1.set_ylabel('Sniffing (a/u)')\n",
    "ax2.set_ylabel('Velocity (m/s)')\n",
    "ax1.set_ylim(-3.5,np.max(sniff.data)+0.2)\n",
    "ax1.xaxis.set_visible(False)\n",
    "ax2.set_ylim(0, speed['filtered_velocity'].max())\n",
    "ax2.set_xlabel('Time (s)')\n",
    "\n",
    "vline1 = ax1.axvline(x=start_time, color='crimson', linestyle='solid')\n",
    "vline2 = ax2.axvline(x=start_time, color='crimson', linestyle='solid')\n",
    "\n",
    "# === Draw all event spans once ===\n",
    "for i, e in events_df.iterrows():\n",
    "    start, end = i, e['stop_time']\n",
    "    if end < start_time - window or start > end_time + window:\n",
    "        continue  # Skip out-of-range events\n",
    "    if e['label'] == 'InterSite': color = '#808080'\n",
    "    elif e['label'] == 'InterPatch': color = '#b3b3b3'\n",
    "    elif e['label'] == 'OdorSite': \n",
    "        color = '#d95f02'\n",
    "    else: continue  # Skip unknown labels\n",
    "\n",
    "    ax1.axvspan(start, end, color=color, alpha=0.6)\n",
    "    ax2.axvspan(start, end, color=color, alpha=0.6)\n",
    "\n",
    "licks = ax2.scatter([], [], color='black', marker='|', s=500)\n",
    "tones = ax2.scatter([], [], color='crimson', marker='s', s=500)\n",
    "rewards = ax2.scatter([], [], color='steelblue', marker='o', s=500)\n",
    "\n",
    "## === Draw the visual corridor ===\n",
    "session_info = data['config'].streams.session_input.data\n",
    "\n",
    "# ------------------ Textures ---------------------\n",
    "remote_textures_root = f\"https://github.com/AllenNeuralDynamics/Aind.Behavior.VrForaging/tree/{session_info['commit_hash']}/src/Textures\"\n",
    "texture_name = \"Floor.jpg\"\n",
    "raw_url = remote_textures_root.replace(\"tree\", \"raw\") + f\"/{texture_name}\"\n",
    "response = requests.get(raw_url)\n",
    "response.raise_for_status()  # Raise an error if the request failed\n",
    "image = np.array(Image.open(BytesIO(response.content)).rotate(90, expand=True))\n",
    "image = image[::5, ::5, :]  # Downsample\n",
    "texture_size = image.shape[1]\n",
    "\n",
    "# Settings\n",
    "visual_corridor_scale = 5 * (end_time-start_time)  # cm per texture repeat\n",
    "\n",
    "# Get position values at the start and end of video\n",
    "current_position = data['operation_control'].streams.CurrentPosition.data.copy()\n",
    "current_position.index = current_position.index - first_time\n",
    "start_pos = current_position.loc[current_position.index >= start_time-window/2].iloc[0]['Position']\n",
    "end_pos = current_position.loc[current_position.index >= end_time+window/2].iloc[0]['Position']\n",
    "win_abs = (start_pos, end_pos)\n",
    "print(win_abs)\n",
    "if win_abs[0] == win_abs[1]:\n",
    "    print(\"❌ Start and end positions are the same. Exiting.\")\n",
    "    exit(1)\n",
    "    \n",
    "# Compute fractional overlap at the start and end\n",
    "first_texture_idx = (1 - ((start_pos / visual_corridor_scale) % 1)) * texture_size\n",
    "last_texture_idx = ((end_pos / visual_corridor_scale) % 1) * texture_size\n",
    "\n",
    "# Clip the first and last texture pieces\n",
    "first_texture = image[:, int(first_texture_idx):, :]\n",
    "last_texture = image[:, :int(last_texture_idx), :]\n",
    "\n",
    "# Compute how many full textures are in between\n",
    "n_textures = int(np.floor((end_pos - start_pos) / visual_corridor_scale))\n",
    "\n",
    "# Repeat the full textures\n",
    "middle_textures = np.tile(image, (1, n_textures, 1))\n",
    "\n",
    "# Combine them\n",
    "stitched_image = np.concatenate((first_texture, middle_textures, last_texture), axis=1)\n",
    "\n",
    "# === OPTIONAL: crop to the exact pixel width corresponding to spatial range ===\n",
    "# Compute the expected width in pixels to match spatial coverage\n",
    "spatial_width = end_pos - start_pos\n",
    "pixels_per_cm = image.shape[1] / visual_corridor_scale\n",
    "expected_pixel_width = int(spatial_width * pixels_per_cm)\n",
    "\n",
    "# Crop to match exactly\n",
    "stitched_image = stitched_image[:, :expected_pixel_width, :]\n",
    "\n",
    "# Function to map position to pixel\n",
    "texture_current_position = lambda pos: (pos - start_pos) * pixels_per_cm\n",
    "\n",
    "events_df['stop_position'] = events_df['start_position'] + events_df['length']\n",
    "events_df = events_df.iloc[:-1]\n",
    "\n",
    "# Make a copy of the original stitched image to apply gray regions\n",
    "gray_overlay_image = stitched_image.copy()\n",
    "\n",
    "for _, row in events_df.iterrows():\n",
    "    if row['label'] not in ['InterSite', 'OdorSite']:\n",
    "        continue\n",
    "    \n",
    "    if row['stop_position'] < win_abs[0] or row['start_position'] > win_abs[1]:\n",
    "        continue  # Fully outside\n",
    "\n",
    "    start_grey = max(row['start_position'], win_abs[0])\n",
    "    stop_grey  = min(row['stop_position'], win_abs[1])\n",
    "    \n",
    "    if start_grey < win_abs[0] or stop_grey > win_abs[1]:\n",
    "        continue  # Skip regions completely outside the window\n",
    "    \n",
    "    pos_gray_start = texture_current_position(start_grey)\n",
    "    pos_gray_end   = texture_current_position(stop_grey)\n",
    "    FACTOR = 0.3\n",
    "\n",
    "    start_idx = int(pos_gray_start)\n",
    "    end_idx   = int(pos_gray_end)\n",
    "\n",
    "    roi = gray_overlay_image[:, start_idx:end_idx, :]\n",
    "    roi = np.clip((roi - 127.5) * FACTOR + 127.5, 0, 255).astype(np.uint8)\n",
    "    gray_overlay_image[:, start_idx:end_idx, :] = roi\n",
    "\n",
    "# Finally, assign back to stitched_image if needed:\n",
    "stitched_image = gray_overlay_image\n",
    "ax3.imshow(stitched_image)\n",
    "\n",
    "# Initial rectangle mask (start by covering everything to the right of current position)\n",
    "start_time_pos = current_position.loc[current_position.index >= start_time].iloc[0]['Position']\n",
    "initial_pos = texture_current_position(start_time_pos)\n",
    "mask_rect = Rectangle(\n",
    "    (initial_pos, 0),\n",
    "    stitched_image.shape[1] - initial_pos,\n",
    "    stitched_image.shape[0],\n",
    "    facecolor=\"white\",\n",
    "    edgecolor=\"white\",\n",
    "    zorder=10 \n",
    ")\n",
    "ax3.add_patch(mask_rect)\n",
    "\n",
    "vline3 = ax3.axvline(x=initial_pos, color='crimson', linestyle='solid', linewidth=5, zorder=100)\n",
    "\n",
    "ax3.set_xlabel('Position (cm)')\n",
    "ax3.axis(\"off\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (plot_w, plot_h * 2))\n",
    "\n",
    "# === Main loop ===\n",
    "for i, row in sync.iterrows():\n",
    "    if i % frame_skip != 0:\n",
    "        continue\n",
    "    \n",
    "    print(np.around(i/len(sync), 2)*100, end=\"\\r\")\n",
    "    current_time = row['ReferenceTime']\n",
    "    frame_num = int(row['Frame'])\n",
    "\n",
    "    # Set video position\n",
    "    cap_face.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "    cap_side.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "    ret_face, frame_face = cap_face.read()\n",
    "    ret_side, frame_side = cap_side.read()\n",
    "\n",
    "    if not ret_face or not ret_side:\n",
    "        print(f\"❌ Failed to read frame {frame_num}\")\n",
    "        break\n",
    "\n",
    "    frame_face = cv2.resize(frame_face, (frame_w, frame_h))\n",
    "    frame_side = cv2.resize(frame_side, (frame_w, frame_h))\n",
    "    # === Check for OdorSite ===\n",
    "    ongoing_odor_events = events_df[\n",
    "        (events_df['label'] == 'OdorSite') &\n",
    "        (events_df.index <= current_time) &\n",
    "        (events_df['stop_time'] >= current_time)\n",
    "    ]\n",
    "\n",
    "    if not ongoing_odor_events.empty:\n",
    "        square_size = 50  # Size of the square in pixels\n",
    "        square_color = (0, 165, 255)  # Orange in BGR\n",
    "        square_thickness = -1  # Filled\n",
    "\n",
    "        top_left = (10, 10)\n",
    "        bottom_right = (top_left[0] + square_size, top_left[1] + square_size)\n",
    "        cv2.rectangle(frame_side, top_left, bottom_right, square_color, square_thickness)\n",
    "    combined_video = np.hstack((frame_side, frame_face))\n",
    "\n",
    "    # Slice data\n",
    "    sniff_slice = sniff[(sniff.index >= (current_time - window/2)) & (sniff.index <= (current_time + window/2))]\n",
    "    speed_slice = speed[(speed.index >= (current_time - window/2)) & (speed.index <= (current_time + window/2))]\n",
    "\n",
    "    # Update plots\n",
    "    sniff_line.set_data(sniff_slice.index, sniff_slice['data'])\n",
    "    speed_line.set_data(speed_slice.index, speed_slice['filtered_velocity'])\n",
    "    ax1.set_xlim(current_time - window/2, current_time + window/2)\n",
    "    ax2.set_xlim(current_time - window/2, current_time + window/2)\n",
    "    vline1.set_xdata([current_time])\n",
    "    vline2.set_xdata([current_time])\n",
    "\n",
    "    lick_slice = lick[(lick.index >= (current_time - window/2)) & (lick.index <= (current_time + window/2))]\n",
    "    y_positions = np.ones(len(lick_slice)) * 20  # Adjust y-axis location if needed\n",
    "    licks.set_offsets(np.column_stack((lick_slice.index, y_positions)))\n",
    "\n",
    "    tone_slice = tone[(tone.index >= (current_time - window/2)) & (tone.index <= (current_time + window/2))]\n",
    "    y_positions = np.ones(len(tone_slice)) * 30  # Adjust y-axis location if needed\n",
    "    tones.set_offsets(np.column_stack((tone_slice.index, y_positions)))\n",
    "\n",
    "    reward_slice = reward[(reward.index >= (current_time - window/2)) & (reward.index <= (current_time + window/2))]\n",
    "    y_positions = np.ones(len(reward_slice)) * 30  # Adjust y-axis location if needed\n",
    "    rewards.set_offsets(np.column_stack((reward_slice.index, y_positions)))\n",
    "    \n",
    "    # Get current position of the animal\n",
    "    if current_time in current_position.index:\n",
    "        pos = current_position.loc[current_time]['Position']\n",
    "    else:\n",
    "        pos = current_position.iloc[current_position.index.get_indexer([current_time], method='nearest')[0]]['Position']\n",
    "    \n",
    "    # Update rectangle mask to match current position\n",
    "    mask_x = texture_current_position(pos)\n",
    "    mask_rect.set_x(mask_x)\n",
    "    mask_rect.set_width(stitched_image.shape[1] - mask_x)\n",
    "    vline3.set_xdata([mask_x])\n",
    "    \n",
    "    canvas.draw()\n",
    "    buf = canvas.buffer_rgba()\n",
    "    plot_img = np.asarray(buf)[:, :, :3].copy()\n",
    "    plot_bgr = cv2.cvtColor(plot_img, cv2.COLOR_RGB2BGR)\n",
    "    plot_resized = cv2.resize(plot_bgr, (plot_w, plot_h))\n",
    "    final_frame = np.vstack((combined_video, plot_resized))\n",
    "    out.write(final_frame)\n",
    "\n",
    "# === Cleanup ===\n",
    "cap_face.release()\n",
    "cap_side.release()\n",
    "out.release()\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Compressing video...\")\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-y\",\n",
    "    \"-i\", output_path,\n",
    "    \"-vcodec\", \"libx264\",\n",
    "    \"-crf\", \"23\",  # Lower = better quality, larger file. Try 23–28\n",
    "    \"-preset\", \"slow\",  # or \"fast\", \"medium\"\n",
    "    f\"compressed_{output_path}\"\n",
    "])\n",
    "\n",
    "os.remove(output_path)\n",
    "\n",
    "print(f\"✅ Video saved to compressed_{output_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
